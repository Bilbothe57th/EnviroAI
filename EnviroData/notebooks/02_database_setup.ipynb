{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa0d3a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environmental Vector Database Setup using ChromaDB and OpenAI\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up OpenAI API (make sure you have your API key set)\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddf6e3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvironmentalVectorDB:\n",
    "    def __init__(self, db_path: str = \"./chroma_env_db\", collection_name: str = \"environmental_data\"):\n",
    "        \"\"\"\n",
    "        Initialize the ChromaDB vector database for environmental data with flexible embedding options\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize ChromaDB client\n",
    "        self.client = chromadb.PersistentClient(\n",
    "            path=db_path,\n",
    "            settings=Settings(\n",
    "                anonymized_telemetry=False\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Create or get collection with specified embeddings\n",
    "        try:\n",
    "            self.collection = self.client.get_collection(\n",
    "                name=collection_name,\n",
    "            )\n",
    "            print(f\"Retrieved existing collection '{collection_name}'\")\n",
    "        except:\n",
    "            self.collection = self.client.create_collection(\n",
    "                name=collection_name,\n",
    "                metadata={\"description\": \"Environmental data for carbon footprint analysis\"}\n",
    "            )\n",
    "            print(f\"Created new collection '{collection_name}'\")\n",
    "        \n",
    "        print(f\"ChromaDB initialized at {db_path}\")\n",
    "        print(f\"Collection '{collection_name}' ready with {self.collection.count()} documents\")\n",
    "    \n",
    "    def add_bulk_data_from_loaders(self, excel_datasets=None, pdf_datasets=None, text_corpus=None):\n",
    "        \"\"\"\n",
    "        Add all data from the data loading functions (Excel, PDF, and text corpus)\n",
    "        \n",
    "        Args:\n",
    "            excel_datasets: Dictionary of Excel DataFrames from load_excel_data()\n",
    "            pdf_datasets: Dictionary of PDF extracted data from load_pdf_extracted_data()\n",
    "            text_corpus: List of text corpus documents from create_text_corpus_for_rag()\n",
    "        \"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"ADDING BULK DATA TO ENVIRONMENTAL VECTOR DATABASE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        total_added = 0\n",
    "        \n",
    "        # Process Excel datasets\n",
    "        if excel_datasets:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(\"PROCESSING EXCEL DATASETS\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            for dataset_name, df in excel_datasets.items():\n",
    "                try:\n",
    "                    print(f\"\\nProcessing Excel dataset: {dataset_name}\")\n",
    "                    added = self._add_excel_dataframe(df, dataset_name)\n",
    "                    total_added += added\n",
    "                    print(f\"✅ Added {added} documents from {dataset_name}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Error processing Excel dataset {dataset_name}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        # Process PDF datasets\n",
    "        if pdf_datasets:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(\"PROCESSING PDF EXTRACTED DATASETS\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            for dataset_name, data in pdf_datasets.items():\n",
    "                try:\n",
    "                    print(f\"\\nProcessing PDF dataset: {dataset_name}\")\n",
    "                    added = self._add_pdf_extracted_data(data, dataset_name)\n",
    "                    total_added += added\n",
    "                    print(f\"✅ Added {added} documents from {dataset_name}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Error processing PDF dataset {dataset_name}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        # Process text corpus\n",
    "        if text_corpus:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(\"PROCESSING TEXT CORPUS\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            try:\n",
    "                added = self._add_text_corpus(text_corpus)\n",
    "                total_added += added\n",
    "                print(f\"✅ Added {added} documents from text corpus\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error processing text corpus: {str(e)}\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"BULK DATA LOADING COMPLETE\")\n",
    "        print(f\"Total documents added: {total_added}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        return total_added\n",
    "    \n",
    "    def _add_excel_dataframe(self, df, dataset_name):\n",
    "        \"\"\"\n",
    "        Internal method to add Excel DataFrame to vector database\n",
    "        \"\"\"\n",
    "        documents = []\n",
    "        metadatas = []\n",
    "        ids = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            # Create comprehensive text content from all columns\n",
    "            text_parts = []\n",
    "            for col in df.columns:\n",
    "                if pd.notna(row[col]) and str(row[col]).strip():\n",
    "                    text_parts.append(f\"{col}: {str(row[col])}\")\n",
    "            \n",
    "            if text_parts:  # Only add if there's actual content\n",
    "                document_text = \" | \".join(text_parts)\n",
    "                documents.append(document_text)\n",
    "                \n",
    "                # Create metadata\n",
    "                metadata = {}\n",
    "                for col in df.columns:\n",
    "                    if pd.notna(row[col]):\n",
    "                        value = row[col]\n",
    "                        # Convert numpy types to Python types\n",
    "                        if isinstance(value, (np.integer, np.floating)):\n",
    "                            value = value.item()\n",
    "                        elif hasattr(value, 'item'):  # Handle other numpy types\n",
    "                            try:\n",
    "                                value = value.item()\n",
    "                            except:\n",
    "                                value = str(value)\n",
    "                         # Convert datetime to string for ChromaDB compatibility\n",
    "                        elif hasattr(value, 'strftime'):  # datetime objects\n",
    "                            value = str(value)\n",
    "                        metadata[col] = value\n",
    "                \n",
    "                metadata['source'] = f\"excel_{dataset_name}\"\n",
    "                metadata['data_type'] = 'excel'\n",
    "                metadata['dataset_name'] = dataset_name\n",
    "                metadatas.append(metadata)\n",
    "                \n",
    "                # Create unique ID\n",
    "                ids.append(f\"excel_{dataset_name}_{idx}\")\n",
    "        \n",
    "        # Add to vector database\n",
    "        if documents:\n",
    "            self.collection.add(\n",
    "                documents=documents,\n",
    "                metadatas=metadatas,\n",
    "                ids=ids\n",
    "            )\n",
    "            return len(documents)\n",
    "        return 0\n",
    "    \n",
    "    def _add_pdf_extracted_data(self, data, dataset_name):\n",
    "        \"\"\"\n",
    "        Internal method to add PDF extracted data to vector database\n",
    "        \"\"\"\n",
    "        documents = []\n",
    "        metadatas = []\n",
    "        ids = []\n",
    "        \n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            # Handle DataFrame from PDF (similar to Excel)\n",
    "            for idx, row in data.iterrows():\n",
    "                text_parts = []\n",
    "                for col in data.columns:\n",
    "                    if pd.notna(row[col]) and str(row[col]).strip():\n",
    "                        text_parts.append(f\"{col}: {str(row[col])}\")\n",
    "                \n",
    "                if text_parts:\n",
    "                    document_text = \" | \".join(text_parts)\n",
    "                    documents.append(document_text)\n",
    "                    \n",
    "                    metadata = {col: row[col] for col in data.columns if pd.notna(row[col])}\n",
    "                    metadata['source'] = f\"pdf_{dataset_name}\"\n",
    "                    metadata['data_type'] = 'pdf_tabular'\n",
    "                    metadata['dataset_name'] = dataset_name\n",
    "                    metadatas.append(metadata)\n",
    "                    \n",
    "                    ids.append(f\"pdf_table_{dataset_name}_{idx}\")\n",
    "        \n",
    "        elif isinstance(data, str):\n",
    "            # Handle text content from PDF - chunk for better retrieval\n",
    "            chunk_size = 1000\n",
    "            overlap = 200\n",
    "            \n",
    "            text = data.strip()\n",
    "            if text:\n",
    "                # Create chunks with overlap\n",
    "                for i in range(0, len(text), chunk_size - overlap):\n",
    "                    chunk = text[i:i + chunk_size]\n",
    "                    if chunk.strip():  # Only add non-empty chunks\n",
    "                        documents.append(chunk)\n",
    "                        \n",
    "                        metadata = {\n",
    "                            'source': f\"pdf_{dataset_name}\",\n",
    "                            'data_type': 'pdf_text',\n",
    "                            'dataset_name': dataset_name,\n",
    "                            'chunk_index': len(documents) - 1,\n",
    "                            'chunk_start': i,\n",
    "                            'chunk_end': min(i + chunk_size, len(text))\n",
    "                        }\n",
    "                        metadatas.append(metadata)\n",
    "                        ids.append(f\"pdf_text_{dataset_name}_chunk_{len(documents) - 1}\")\n",
    "        \n",
    "        elif isinstance(data, dict):\n",
    "            # Handle JSON/dict data from PDF\n",
    "            text_content = f\"Document: {dataset_name}\\n\"\n",
    "            text_content += json.dumps(data, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            documents.append(text_content)\n",
    "            metadata = {\n",
    "                'source': f\"pdf_{dataset_name}\",\n",
    "                'data_type': 'pdf_json',\n",
    "                'dataset_name': dataset_name,\n",
    "                'keys': list(data.keys()) if isinstance(data, dict) else []\n",
    "            }\n",
    "            metadatas.append(metadata)\n",
    "            ids.append(f\"pdf_json_{dataset_name}\")\n",
    "        \n",
    "        elif isinstance(data, list):\n",
    "            # Handle list data from PDF\n",
    "            for i, item in enumerate(data):\n",
    "                text_content = f\"Item {i} from {dataset_name}: {str(item)}\"\n",
    "                documents.append(text_content)\n",
    "                \n",
    "                metadata = {\n",
    "                    'source': f\"pdf_{dataset_name}\",\n",
    "                    'data_type': 'pdf_list',\n",
    "                    'dataset_name': dataset_name,\n",
    "                    'item_index': i\n",
    "                }\n",
    "                metadatas.append(metadata)\n",
    "                ids.append(f\"pdf_list_{dataset_name}_{i}\")\n",
    "        \n",
    "        # Add to vector database\n",
    "        if documents:\n",
    "            self.collection.add(\n",
    "                documents=documents,\n",
    "                metadatas=metadatas,\n",
    "                ids=ids\n",
    "            )\n",
    "            return len(documents)\n",
    "        return 0\n",
    "\n",
    "    def search_similar_with_scores(self, query: str, n_results: int = 5, \n",
    "                                  score_threshold: float = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Enhanced search with similarity scores and optional filtering\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            n_results: Number of results to return\n",
    "            score_threshold: Optional minimum similarity score (0-1, higher is more similar)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing search results with similarity scores\n",
    "        \"\"\"\n",
    "        try:\n",
    "            results = self.collection.query(\n",
    "                query_texts=[query],\n",
    "                n_results=n_results\n",
    "            )\n",
    "            \n",
    "            # Convert distances to similarity scores (1 - distance)\n",
    "            similarities = [1 - d for d in results['distances'][0]] if results['distances'] else []\n",
    "            \n",
    "            # Filter by score threshold if provided\n",
    "            if score_threshold is not None:\n",
    "                filtered_results = {\n",
    "                    'documents': [],\n",
    "                    'metadatas': [],\n",
    "                    'similarities': [],\n",
    "                    'ids': []\n",
    "                }\n",
    "                \n",
    "                for i, similarity in enumerate(similarities):\n",
    "                    if similarity >= score_threshold:\n",
    "                        filtered_results['documents'].append(results['documents'][0][i])\n",
    "                        filtered_results['metadatas'].append(results['metadatas'][0][i])\n",
    "                        filtered_results['similarities'].append(similarity)\n",
    "                        filtered_results['ids'].append(results['ids'][0][i])\n",
    "                \n",
    "                return filtered_results\n",
    "            \n",
    "            return {\n",
    "                'documents': results['documents'][0] if results['documents'] else [],\n",
    "                'metadatas': results['metadatas'][0] if results['metadatas'] else [],\n",
    "                'similarities': similarities,\n",
    "                'distances': results['distances'][0] if results['distances'] else [],\n",
    "                'ids': results['ids'][0] if results['ids'] else []\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error searching database: {str(e)}\")\n",
    "            return {}\n",
    "        \n",
    "    def query_environmental_data(self, user_question: str, max_results: int = 5, \n",
    "                                score_threshold: float = 0.2) -> dict:\n",
    "        \"\"\"\n",
    "        Enhanced query method specifically designed for chatbot integration\n",
    "        Query the environmental database for carbon footprint information\n",
    "        \n",
    "        Args:\n",
    "            user_question: User's question about environmental impact\n",
    "            max_results: Maximum number of results to return\n",
    "            score_threshold: Minimum similarity threshold (0-1, higher is more similar)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with search results and metadata formatted for chatbot use\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Search the vector database\n",
    "            results = self.search_similar_with_scores(\n",
    "                query=user_question,\n",
    "                n_results=max_results,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "            \n",
    "            if not results.get('documents'):\n",
    "                return {\n",
    "                    'success': False,\n",
    "                    'message': 'No relevant environmental data found for your question.',\n",
    "                    'query': user_question,\n",
    "                    'results': []\n",
    "                }\n",
    "            \n",
    "            # Format results for chatbot use\n",
    "            formatted_results = []\n",
    "            for doc, metadata, similarity in zip(\n",
    "                results['documents'],\n",
    "                results['metadatas'],\n",
    "                results['similarities']\n",
    "            ):\n",
    "                formatted_results.append({\n",
    "                    'content': doc,\n",
    "                    'source': metadata.get('source', 'Unknown'),\n",
    "                    'data_type': metadata.get('data_type', 'Unknown'),\n",
    "                    'dataset_name': metadata.get('dataset_name', 'Unknown'),\n",
    "                    'similarity_score': round(similarity, 3),\n",
    "                    'metadata': {k: v for k, v in metadata.items() \n",
    "                               if k not in ['source', 'data_type', 'dataset_name']}\n",
    "                })\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'query': user_question,\n",
    "                'results': formatted_results,\n",
    "                'total_results': len(formatted_results)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'message': f'Error querying database: {str(e)}',\n",
    "                'query': user_question,\n",
    "                'results': []\n",
    "            }\n",
    "\n",
    "    def test_environmental_queries(self, test_queries=None):\n",
    "        \"\"\"\n",
    "        Test the vector database with sample environmental queries\n",
    "        \n",
    "        Args:\n",
    "            test_queries: Optional list of custom test queries\n",
    "        \"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"TESTING ENVIRONMENTAL VECTOR DATABASE QUERIES\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Default test queries if none provided\n",
    "        if test_queries is None:\n",
    "            test_queries = [\n",
    "                \"carbon emissions from transportation\",\n",
    "                \"energy consumption and CO2\",\n",
    "                \"renewable energy sources\",\n",
    "                \"manufacturing environmental impact\",\n",
    "                \"agricultural carbon footprint\",\n",
    "                \"building energy efficiency\",\n",
    "                \"waste management emissions\",\n",
    "                \"aviation fuel consumption\"\n",
    "            ]\n",
    "        \n",
    "        for i, query in enumerate(test_queries, 1):\n",
    "            print(f\"\\n{'-'*50}\")\n",
    "            print(f\"QUERY {i}: '{query}'\")\n",
    "            print(f\"{'-'*50}\")\n",
    "            \n",
    "            # Use the chatbot query method\n",
    "            result = self.query_environmental_data(query, max_results=3)\n",
    "            \n",
    "            if result['success'] and result['results']:\n",
    "                print(f\"Found {result['total_results']} relevant results:\\n\")\n",
    "                \n",
    "                for j, res in enumerate(result['results'], 1):\n",
    "                    print(f\"  Result {j} (Similarity: {res['similarity_score']}):\")\n",
    "                    print(f\"    Source: {res['source']}\")\n",
    "                    print(f\"    Dataset: {res['dataset_name']}\")\n",
    "                    print(f\"    Type: {res['data_type']}\")\n",
    "                    \n",
    "                    # Show preview of document\n",
    "                    doc_preview = res['content'][:200] + \"...\" if len(res['content']) > 200 else res['content']\n",
    "                    print(f\"    Content: {doc_preview}\")\n",
    "                    \n",
    "                    # Show relevant metadata\n",
    "                    if res['metadata']:\n",
    "                        relevant_metadata = {k: v for k, v in res['metadata'].items() \n",
    "                                          if k in ['category', 'emission_factor', 'unit'] and v}\n",
    "                        if relevant_metadata:\n",
    "                            print(f\"    Metadata: {relevant_metadata}\")\n",
    "                    print()\n",
    "            else:\n",
    "                print(f\"  {result['message']}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e01e1e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved existing collection 'carbon_footprint_data'\n",
      "ChromaDB initialized at ./chroma_env_db\n",
      "Collection 'carbon_footprint_data' ready with 2500 documents\n"
     ]
    }
   ],
   "source": [
    "env_db = EnvironmentalVectorDB(\n",
    "    db_path=\"./chroma_env_db\",\n",
    "    collection_name=\"carbon_footprint_data\"\n",
    ")\n",
    "#chroma run --host localhost --port 8000 --path ./chroma_env_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "373fcc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ADDING BULK DATA TO ENVIRONMENTAL VECTOR DATABASE\n",
      "============================================================\n",
      "\n",
      "==================================================\n",
      "PROCESSING EXCEL DATASETS\n",
      "==================================================\n",
      "\n",
      "Processing Excel dataset: ghg-conversion-factors-2023-condensed-set-update_Introduction\n",
      "✅ Added 38 documents from ghg-conversion-factors-2023-condensed-set-update_Introduction\n",
      "\n",
      "Processing Excel dataset: ghg-conversion-factors-2023-condensed-set-update_What's new\n",
      "✅ Added 36 documents from ghg-conversion-factors-2023-condensed-set-update_What's new\n",
      "\n",
      "Processing Excel dataset: ghg-conversion-factors-2023-condensed-set-update_Index\n",
      "✅ Added 95 documents from ghg-conversion-factors-2023-condensed-set-update_Index\n",
      "\n",
      "Processing Excel dataset: ghg-conversion-factors-2023-condensed-set-update_Fuels\n",
      "✅ Added 145 documents from ghg-conversion-factors-2023-condensed-set-update_Fuels\n",
      "\n",
      "Processing Excel dataset: ghg-conversion-factors-2023-condensed-set-update_Bioenergy\n",
      "✅ Added 70 documents from ghg-conversion-factors-2023-condensed-set-update_Bioenergy\n",
      "\n",
      "Processing Excel dataset: ghg-conversion-factors-2023-condensed-set-update_Refrigerant & other\n",
      "✅ Added 197 documents from ghg-conversion-factors-2023-condensed-set-update_Refrigerant & other\n",
      "\n",
      "Processing Excel dataset: ghg-conversion-factors-2023-condensed-set-update_Passenger vehicles\n",
      "✅ Added 102 documents from ghg-conversion-factors-2023-condensed-set-update_Passenger vehicles\n",
      "\n",
      "Processing Excel dataset: ghg-conversion-factors-2023-condensed-set-update_SECR kWh pass & delivery vehs\n",
      "✅ Added 119 documents from ghg-conversion-factors-2023-condensed-set-update_SECR kWh pass & delivery vehs\n",
      "\n",
      "Processing Excel dataset: ghg-conversion-factors-2023-condensed-set-update_UK electricity\n",
      "✅ Added 24 documents from ghg-conversion-factors-2023-condensed-set-update_UK electricity\n",
      "\n",
      "Processing Excel dataset: ghg-conversion-factors-2023-condensed-set-update_UK electricity for EVs\n",
      "✅ Added 71 documents from ghg-conversion-factors-2023-condensed-set-update_UK electricity for EVs\n",
      "\n",
      "Processing Excel dataset: ghg-conversion-factors-2023-condensed-set-update_SECR kWh UK electricity for EVs\n",
      "✅ Added 65 documents from ghg-conversion-factors-2023-condensed-set-update_SECR kWh UK electricity for EVs\n",
      "\n",
      "Processing Excel dataset: ghg-conversion-factors-2023-condensed-set-update_Transmission and distribution\n",
      "✅ Added 23 documents from ghg-conversion-factors-2023-condensed-set-update_Transmission and distribution\n",
      "\n",
      "Processing Excel dataset: ghg-conversion-factors-2023-condensed-set-update_UK electricity T&D for EVs\n",
      "✅ Added 69 documents from ghg-conversion-factors-2023-condensed-set-update_UK electricity T&D for EVs\n",
      "\n",
      "Processing Excel dataset: ghg-conversion-factors-2023-condensed-set-update_Water supply\n",
      "✅ Added 14 documents from ghg-conversion-factors-2023-condensed-set-update_Water supply\n",
      "\n",
      "Processing Excel dataset: ghg-conversion-factors-2023-condensed-set-update_Water treatment\n",
      "✅ Added 14 documents from ghg-conversion-factors-2023-condensed-set-update_Water treatment\n",
      "\n",
      "Processing Excel dataset: ghg-conversion-factors-2023-condensed-set-update_Material use\n",
      "✅ Added 72 documents from ghg-conversion-factors-2023-condensed-set-update_Material use\n",
      "\n",
      "Processing Excel dataset: ghg-conversion-factors-2023-condensed-set-update_Waste disposal\n",
      "✅ Added 78 documents from ghg-conversion-factors-2023-condensed-set-update_Waste disposal\n",
      "\n",
      "Processing Excel dataset: ghg-conversion-factors-2023-condensed-set-update_Business travel- air\n",
      "✅ Added 46 documents from ghg-conversion-factors-2023-condensed-set-update_Business travel- air\n",
      "\n",
      "Processing Excel dataset: ghg-conversion-factors-2023-condensed-set-update_Business travel- sea\n",
      "✅ Added 18 documents from ghg-conversion-factors-2023-condensed-set-update_Business travel- sea\n",
      "\n",
      "Processing Excel dataset: ghg-conversion-factors-2023-condensed-set-update_Business travel- land\n",
      "✅ Added 78 documents from ghg-conversion-factors-2023-condensed-set-update_Business travel- land\n",
      "\n",
      "Processing Excel dataset: ghg-conversion-factors-2023-condensed-set-update_Freighting goods\n",
      "✅ Added 159 documents from ghg-conversion-factors-2023-condensed-set-update_Freighting goods\n",
      "\n",
      "Processing Excel dataset: ghg-conversion-factors-2023-condensed-set-update_Hotel stay\n",
      "✅ Added 77 documents from ghg-conversion-factors-2023-condensed-set-update_Hotel stay\n",
      "\n",
      "Processing Excel dataset: ghg-conversion-factors-2023-condensed-set-update_Homeworking\n",
      "✅ Added 20 documents from ghg-conversion-factors-2023-condensed-set-update_Homeworking\n",
      "\n",
      "Processing Excel dataset: ghg-conversion-factors-2023-condensed-set-update_Conversions\n",
      "✅ Added 47 documents from ghg-conversion-factors-2023-condensed-set-update_Conversions\n",
      "\n",
      "Processing Excel dataset: ghg-conversion-factors-2023-condensed-set-update_Fuel properties\n",
      "✅ Added 54 documents from ghg-conversion-factors-2023-condensed-set-update_Fuel properties\n",
      "\n",
      "Processing Excel dataset: ghg-conversion-factors-2023-condensed-set-update_Haul definition\n",
      "✅ Added 218 documents from ghg-conversion-factors-2023-condensed-set-update_Haul definition\n",
      "\n",
      "Processing Excel dataset: ghg-emission-factors-hub-2025\n",
      "✅ Added 536 documents from ghg-emission-factors-hub-2025\n",
      "\n",
      "==================================================\n",
      "PROCESSING PDF EXTRACTED DATASETS\n",
      "==================================================\n",
      "\n",
      "Processing PDF dataset: ZyPDF\n",
      "✅ Added 15 documents from ZyPDF\n",
      "\n",
      "============================================================\n",
      "BULK DATA LOADING COMPLETE\n",
      "Total documents added: 2500\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_db.add_bulk_data_from_loaders(\n",
    "    excel_datasets=excel_datasets,\n",
    "    pdf_datasets=pdf_datasets, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2b7b370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VITE_OPENAI_API_KEY=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\n",
      "VITE_CHROMA_DB_URL=http://localhost:8000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Export your variables\n",
    "print(f\"VITE_OPENAI_API_KEY={os.getenv('OPENAI_API_KEY')}\")\n",
    "print(f\"VITE_CHROMA_DB_URL={os.getenv('CHROMA_DB_URL', 'http://localhost:8000')}\")\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Start ChromaDB server\n",
    "client = chromadb.HttpClient(\n",
    "    host=\"localhost\",\n",
    "    port=8000,\n",
    "    \n",
    "    settings=Settings(allow_reset=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd021eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTING ENVIRONMENTAL VECTOR DATABASE QUERIES\n",
      "============================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "QUERY 1: 'carbon emissions from transportation'\n",
      "--------------------------------------------------\n",
      "Found 3 relevant results:\n",
      "\n",
      "  Result 1 (Similarity: 0.449):\n",
      "    Source: EPA\n",
      "    Dataset: Unknown\n",
      "    Type: Unknown\n",
      "    Content: Transportation: Cars emit approximately 4.6 metric tons of CO2 per year per vehicle\n",
      "    Metadata: {'emission_factor': 4.6, 'category': 'transportation', 'unit': 'metric tons CO2/year'}\n",
      "\n",
      "  Result 2 (Similarity: 0.354):\n",
      "    Source: pdf_ZyPDF\n",
      "    Dataset: ZyPDF\n",
      "    Type: pdf_text\n",
      "    Content: ion on the emissions of the transportation\n",
      "sector as a whole?\n",
      "Visit EPA’s Fast Facts on Transportation Greenhouse Gas Emissions and Carbon Pollution from\n",
      "Transportation.\n",
      "Annually EPA also publishes in...\n",
      "\n",
      "  Result 3 (Similarity: 0.258):\n",
      "    Source: excel_ghg-emission-factors-hub-2025\n",
      "    Dataset: ghg-emission-factors-hub-2025\n",
      "    Type: excel\n",
      "    Content: Unnamed: 2: Source: \n",
      "CO2, CH4, and N2O emissions data for on-road vehicles are from Table 2-13 of the EPA (2024) Inventory of U.S. Greenhouse Gas Emissions and Sinks: 1990-2022.\n",
      "Vehicle-miles data for...\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "QUERY 2: 'energy consumption and CO2'\n",
      "--------------------------------------------------\n",
      "  No relevant environmental data found for your question.\n",
      "\n",
      "--------------------------------------------------\n",
      "QUERY 3: 'renewable energy sources'\n",
      "--------------------------------------------------\n",
      "  No relevant environmental data found for your question.\n",
      "\n",
      "--------------------------------------------------\n",
      "QUERY 4: 'manufacturing environmental impact'\n",
      "--------------------------------------------------\n",
      "  No relevant environmental data found for your question.\n",
      "\n",
      "--------------------------------------------------\n",
      "QUERY 5: 'agricultural carbon footprint'\n",
      "--------------------------------------------------\n",
      "  No relevant environmental data found for your question.\n",
      "\n",
      "--------------------------------------------------\n",
      "QUERY 6: 'building energy efficiency'\n",
      "--------------------------------------------------\n",
      "  No relevant environmental data found for your question.\n",
      "\n",
      "--------------------------------------------------\n",
      "QUERY 7: 'waste management emissions'\n",
      "--------------------------------------------------\n",
      "Found 3 relevant results:\n",
      "\n",
      "  Result 1 (Similarity: 0.306):\n",
      "    Source: excel_ghg-emission-factors-hub-2025\n",
      "    Dataset: ghg-emission-factors-hub-2025\n",
      "    Type: excel\n",
      "    Content: Unnamed: 2: Notes:\n",
      "These factors do not include avoided emissions impact from any of the disposal methods. This exclusion is an adjustment to the life-cycle factors in the WARM tool. Thus the waste fa...\n",
      "\n",
      "  Result 2 (Similarity: 0.284):\n",
      "    Source: excel_ghg-conversion-factors-2023-condensed-set-update_Waste disposal\n",
      "    Dataset: ghg-conversion-factors-2023-condensed-set-update_Waste disposal\n",
      "    Type: excel\n",
      "    Content: UK Government GHG Conversion Factors for Company Reporting: ●  These factors cannot be used to determine the relative lifecycle merit of different waste management options. This is because the emissio...\n",
      "\n",
      "  Result 3 (Similarity: 0.284):\n",
      "    Source: excel_ghg-conversion-factors-2023-condensed-set-update_Waste disposal\n",
      "    Dataset: ghg-conversion-factors-2023-condensed-set-update_Waste disposal\n",
      "    Type: excel\n",
      "    Content: UK Government GHG Conversion Factors for Company Reporting: Example of calculating emissions from waste disposal\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "QUERY 8: 'aviation fuel consumption'\n",
      "--------------------------------------------------\n",
      "  No relevant environmental data found for your question.\n"
     ]
    }
   ],
   "source": [
    "env_db.test_environmental_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bad9bd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 Excel files:\n",
      "Loading: ghg-conversion-factors-2023-condensed-set-update.xlsx\n",
      "  - Sheet 'Introduction': 43 rows, 4 columns\n",
      "  - Sheet 'What's new': 49 rows, 4 columns\n",
      "  - Sheet 'Index': 103 rows, 4 columns\n",
      "  - Sheet 'Fuels': 159 rows, 8 columns\n",
      "  - Sheet 'Bioenergy': 83 rows, 6 columns\n",
      "  - Sheet 'Refrigerant & other': 229 rows, 7 columns\n",
      "  - Sheet 'Passenger vehicles': 118 rows, 35 columns\n",
      "  - Sheet 'SECR kWh pass & delivery vehs': 137 rows, 11 columns\n",
      "  - Sheet 'UK electricity': 31 rows, 8 columns\n",
      "  - Sheet 'UK electricity for EVs': 86 rows, 11 columns\n",
      "  - Sheet 'SECR kWh UK electricity for EVs': 77 rows, 6 columns\n",
      "  - Sheet 'Transmission and distribution': 32 rows, 8 columns\n",
      "  - Sheet 'UK electricity T&D for EVs': 88 rows, 11 columns\n",
      "  - Sheet 'Water supply': 22 rows, 6 columns\n",
      "  - Sheet 'Water treatment': 19 rows, 6 columns\n",
      "  - Sheet 'Material use': 92 rows, 14 columns\n",
      "  - Sheet 'Waste disposal': 97 rows, 14 columns\n",
      "  - Sheet 'Business travel- air': 55 rows, 12 columns\n",
      "  - Sheet 'Business travel- sea': 24 rows, 7 columns\n",
      "  - Sheet 'Business travel- land': 102 rows, 35 columns\n",
      "  - Sheet 'Freighting goods': 184 rows, 31 columns\n",
      "  - Sheet 'Hotel stay': 90 rows, 7 columns\n",
      "  - Sheet 'Homeworking': 30 rows, 6 columns\n",
      "  - Sheet 'Conversions': 64 rows, 8 columns\n",
      "  - Sheet 'Fuel properties': 62 rows, 11 columns\n",
      "  - Sheet 'Haul definition': 219 rows, 3 columns\n",
      "Loading: ghg-emission-factors-hub-2025.xlsx\n",
      "  - Loaded 590 rows, 11 columns\n",
      "\n",
      "Successfully loaded 27 datasets from Excel files\n",
      "\n",
      "==================================================\n",
      "EXCEL DATA SUMMARY\n",
      "==================================================\n",
      "ghg-conversion-factors-2023-condensed-set-update_Introduction:\n",
      "  Shape: (43, 4)\n",
      "  Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3']\n",
      "  Memory usage: 0.02 MB\n",
      "\n",
      "ghg-conversion-factors-2023-condensed-set-update_What's new:\n",
      "  Shape: (49, 4)\n",
      "  Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3']\n",
      "  Memory usage: 0.01 MB\n",
      "\n",
      "ghg-conversion-factors-2023-condensed-set-update_Index:\n",
      "  Shape: (103, 4)\n",
      "  Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3']\n",
      "  Memory usage: 0.03 MB\n",
      "\n",
      "ghg-conversion-factors-2023-condensed-set-update_Fuels:\n",
      "  Shape: (159, 8)\n",
      "  Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "  Memory usage: 0.06 MB\n",
      "\n",
      "ghg-conversion-factors-2023-condensed-set-update_Bioenergy:\n",
      "  Shape: (83, 6)\n",
      "  Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "  Memory usage: 0.02 MB\n",
      "\n",
      "ghg-conversion-factors-2023-condensed-set-update_Refrigerant & other:\n",
      "  Shape: (229, 7)\n",
      "  Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "  Memory usage: 0.07 MB\n",
      "\n",
      "ghg-conversion-factors-2023-condensed-set-update_Passenger vehicles:\n",
      "  Shape: (118, 35)\n",
      "  Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "  Memory usage: 0.15 MB\n",
      "\n",
      "ghg-conversion-factors-2023-condensed-set-update_SECR kWh pass & delivery vehs:\n",
      "  Shape: (137, 11)\n",
      "  Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "  Memory usage: 0.06 MB\n",
      "\n",
      "ghg-conversion-factors-2023-condensed-set-update_UK electricity:\n",
      "  Shape: (31, 8)\n",
      "  Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "  Memory usage: 0.02 MB\n",
      "\n",
      "ghg-conversion-factors-2023-condensed-set-update_UK electricity for EVs:\n",
      "  Shape: (86, 11)\n",
      "  Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "  Memory usage: 0.04 MB\n",
      "\n",
      "ghg-conversion-factors-2023-condensed-set-update_SECR kWh UK electricity for EVs:\n",
      "  Shape: (77, 6)\n",
      "  Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "  Memory usage: 0.02 MB\n",
      "\n",
      "ghg-conversion-factors-2023-condensed-set-update_Transmission and distribution:\n",
      "  Shape: (32, 8)\n",
      "  Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "  Memory usage: 0.01 MB\n",
      "\n",
      "ghg-conversion-factors-2023-condensed-set-update_UK electricity T&D for EVs:\n",
      "  Shape: (88, 11)\n",
      "  Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "  Memory usage: 0.04 MB\n",
      "\n",
      "ghg-conversion-factors-2023-condensed-set-update_Water supply:\n",
      "  Shape: (22, 6)\n",
      "  Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "  Memory usage: 0.01 MB\n",
      "\n",
      "ghg-conversion-factors-2023-condensed-set-update_Water treatment:\n",
      "  Shape: (19, 6)\n",
      "  Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "  Memory usage: 0.01 MB\n",
      "\n",
      "ghg-conversion-factors-2023-condensed-set-update_Material use:\n",
      "  Shape: (92, 14)\n",
      "  Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "  Memory usage: 0.04 MB\n",
      "\n",
      "ghg-conversion-factors-2023-condensed-set-update_Waste disposal:\n",
      "  Shape: (97, 14)\n",
      "  Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "  Memory usage: 0.05 MB\n",
      "\n",
      "ghg-conversion-factors-2023-condensed-set-update_Business travel- air:\n",
      "  Shape: (55, 12)\n",
      "  Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "  Memory usage: 0.03 MB\n",
      "\n",
      "ghg-conversion-factors-2023-condensed-set-update_Business travel- sea:\n",
      "  Shape: (24, 7)\n",
      "  Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "  Memory usage: 0.01 MB\n",
      "\n",
      "ghg-conversion-factors-2023-condensed-set-update_Business travel- land:\n",
      "  Shape: (102, 35)\n",
      "  Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "  Memory usage: 0.12 MB\n",
      "\n",
      "ghg-conversion-factors-2023-condensed-set-update_Freighting goods:\n",
      "  Shape: (184, 31)\n",
      "  Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "  Memory usage: 0.20 MB\n",
      "\n",
      "ghg-conversion-factors-2023-condensed-set-update_Hotel stay:\n",
      "  Shape: (90, 7)\n",
      "  Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "  Memory usage: 0.03 MB\n",
      "\n",
      "ghg-conversion-factors-2023-condensed-set-update_Homeworking:\n",
      "  Shape: (30, 6)\n",
      "  Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "  Memory usage: 0.01 MB\n",
      "\n",
      "ghg-conversion-factors-2023-condensed-set-update_Conversions:\n",
      "  Shape: (64, 8)\n",
      "  Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "  Memory usage: 0.02 MB\n",
      "\n",
      "ghg-conversion-factors-2023-condensed-set-update_Fuel properties:\n",
      "  Shape: (62, 11)\n",
      "  Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "  Memory usage: 0.03 MB\n",
      "\n",
      "ghg-conversion-factors-2023-condensed-set-update_Haul definition:\n",
      "  Shape: (219, 3)\n",
      "  Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2']\n",
      "  Memory usage: 0.04 MB\n",
      "\n",
      "ghg-emission-factors-hub-2025:\n",
      "  Shape: (590, 11)\n",
      "  Columns: ['Unnamed: 0', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "  Memory usage: 0.23 MB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jordan\\miniconda3\\envs\\env_data_processor\\lib\\site-packages\\openpyxl\\worksheet\\header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n",
      "c:\\Users\\Jordan\\miniconda3\\envs\\env_data_processor\\lib\\site-packages\\openpyxl\\worksheet\\_read_only.py:85: UserWarning: Unknown extension is not supported and will be removed\n",
      "  for idx, row in parser.parse():\n"
     ]
    }
   ],
   "source": [
    "def load_excel_data(processed_dir='data/raw/excel'):\n",
    "    \"\"\"\n",
    "    Load all Excel files from the processed data directory\n",
    "    Returns a dictionary with filename as key and DataFrame as value\n",
    "    \"\"\"\n",
    "    excel_data = {}\n",
    "    excel_extensions = ['*.xlsx', '*.xls', '*.xlsm']\n",
    "    # Find all Excel files\n",
    "    excel_files = []\n",
    "    data_path = Path(processed_dir)\n",
    "    for ext in excel_extensions:\n",
    "        excel_files.extend(data_path.glob(ext))\n",
    "    \n",
    "    if not excel_files:\n",
    "        print(f\"No Excel files found in {processed_dir}\")\n",
    "        return excel_data\n",
    "    \n",
    "    print(f\"Found {len(excel_files)} Excel files:\")\n",
    "    \n",
    "    for file_path in excel_files:\n",
    "        try:\n",
    "            print(f\"Loading: {file_path.name}\")\n",
    "            \n",
    "            # Try to read the Excel file\n",
    "            # Handle multiple sheets by reading all sheets\n",
    "            excel_file = pd.ExcelFile(file_path)\n",
    "            \n",
    "            if len(excel_file.sheet_names) == 1:\n",
    "                # Single sheet - store directly\n",
    "                df = pd.read_excel(file_path, sheet_name=0)\n",
    "                excel_data[file_path.stem] = df\n",
    "                print(f\"  - Loaded {len(df)} rows, {len(df.columns)} columns\")\n",
    "            else:\n",
    "                # Multiple sheets - store each sheet separately\n",
    "                for sheet_name in excel_file.sheet_names:\n",
    "                    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "                    key = f\"{file_path.stem}_{sheet_name}\"\n",
    "                    excel_data[key] = df\n",
    "                    print(f\"  - Sheet '{sheet_name}': {len(df)} rows, {len(df.columns)} columns\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path.name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nSuccessfully loaded {len(excel_data)} datasets from Excel files\")\n",
    "    return excel_data\n",
    "\n",
    "# Load all Excel data\n",
    "excel_datasets = load_excel_data()\n",
    "\n",
    "# Display summary of loaded datasets\n",
    "if excel_datasets:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EXCEL DATA SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    for name, df in excel_datasets.items():\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  Shape: {df.shape}\")\n",
    "        print(f\"  Columns: {list(df.columns[:5])}{'...' if len(df.columns) > 5 else ''}\")\n",
    "        print(f\"  Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe11a974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 extracted PDF data files:\n",
      "Loading: ZyPDF.txt\n",
      "  - Text file: 187 lines, 1814 words, 11426 characters\n",
      "\n",
      "Successfully loaded 1 datasets from extracted PDF files\n",
      "\n",
      "==================================================\n",
      "EXTRACTED PDF DATA SUMMARY\n",
      "==================================================\n",
      "ZyPDF:\n",
      "  Type: Text\n",
      "  Length: 11426 characters\n",
      "  Preview: Questions\n",
      "and\n",
      "Answers\n",
      "Office of Transportation and Air Quality\n",
      "EPA-420-F-23-014\n",
      "June 2023\n",
      "Tailpipe G...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_pdf_extracted_data(processed_dir='data/processed'):\n",
    "    \"\"\"\n",
    "    Load extracted PDF data from the processed data directory\n",
    "    Handles various formats: JSON, CSV, TXT files from PDF extraction\n",
    "    Returns a dictionary with filename as key and content as value\n",
    "    \"\"\"\n",
    "    pdf_data = {}\n",
    "    \n",
    "    # Common extensions for extracted PDF data\n",
    "    pdf_extracted_extensions = ['*.json', '*.csv', '*.txt']\n",
    "    \n",
    "    # Create the directory path\n",
    "    data_path = Path(processed_dir)\n",
    "    \n",
    "    if not data_path.exists():\n",
    "        print(f\"Directory {processed_dir} does not exist!\")\n",
    "        return pdf_data\n",
    "    \n",
    "    # Find all extracted PDF data files\n",
    "    extracted_files = []\n",
    "    for ext in pdf_extracted_extensions:\n",
    "        extracted_files.extend(data_path.glob(ext))\n",
    "    \n",
    "    # Filter out Excel files that might have .csv extension\n",
    "    extracted_files = [f for f in extracted_files if not any(excel_ext in f.name.lower() \n",
    "                                                           for excel_ext in ['.xlsx', '.xls', '.xlsm'])]\n",
    "    \n",
    "    if not extracted_files:\n",
    "        print(f\"No extracted PDF data files found in {processed_dir}\")\n",
    "        return pdf_data\n",
    "    \n",
    "    print(f\"Found {len(extracted_files)} extracted PDF data files:\")\n",
    "    \n",
    "    for file_path in extracted_files:\n",
    "        try:\n",
    "            print(f\"Loading: {file_path.name}\")\n",
    "            \n",
    "            if file_path.suffix.lower() == '.json':\n",
    "                # Load JSON data\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                    pdf_data[file_path.stem] = data\n",
    "                    \n",
    "                    # Try to provide summary based on data structure\n",
    "                    if isinstance(data, dict):\n",
    "                        print(f\"  - JSON object with {len(data)} keys\")\n",
    "                        if 'pages' in data:\n",
    "                            print(f\"  - Contains {len(data['pages'])} pages\")\n",
    "                    elif isinstance(data, list):\n",
    "                        print(f\"  - JSON array with {len(data)} items\")\n",
    "                    \n",
    "            elif file_path.suffix.lower() == '.csv':\n",
    "                # Load CSV data (likely tabular data extracted from PDF)\n",
    "                df = pd.read_csv(file_path)\n",
    "                pdf_data[file_path.stem] = df\n",
    "                print(f\"  - CSV with {len(df)} rows, {len(df.columns)} columns\")\n",
    "                \n",
    "            elif file_path.suffix.lower() == '.txt':\n",
    "                # Load text data\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text_content = f.read()\n",
    "                    pdf_data[file_path.stem] = text_content\n",
    "                    \n",
    "                    # Provide text summary\n",
    "                    lines = len(text_content.split('\\n'))\n",
    "                    chars = len(text_content)\n",
    "                    words = len(text_content.split())\n",
    "                    print(f\"  - Text file: {lines} lines, {words} words, {chars} characters\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path.name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nSuccessfully loaded {len(pdf_data)} datasets from extracted PDF files\")\n",
    "    return pdf_data\n",
    "\n",
    "# Load all extracted PDF data\n",
    "pdf_datasets = load_pdf_extracted_data()\n",
    "\n",
    "# Display summary of loaded datasets\n",
    "if pdf_datasets:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EXTRACTED PDF DATA SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    for name, data in pdf_datasets.items():\n",
    "        print(f\"{name}:\")\n",
    "        \n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            print(f\"  Type: DataFrame\")\n",
    "            print(f\"  Shape: {data.shape}\")\n",
    "            print(f\"  Columns: {list(data.columns[:3])}{'...' if len(data.columns) > 3 else ''}\")\n",
    "        elif isinstance(data, dict):\n",
    "            print(f\"  Type: Dictionary\")\n",
    "            print(f\"  Keys: {list(data.keys())[:5]}{'...' if len(data.keys()) > 5 else ''}\")\n",
    "        elif isinstance(data, str):\n",
    "            print(f\"  Type: Text\")\n",
    "            print(f\"  Length: {len(data)} characters\")\n",
    "            print(f\"  Preview: {data[:100]}{'...' if len(data) > 100 else ''}\")\n",
    "        elif isinstance(data, list):\n",
    "            print(f\"  Type: List\")\n",
    "            print(f\"  Items: {len(data)}\")\n",
    "        else:\n",
    "            print(f\"  Type: {type(data).__name__}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "467381d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created text corpus with 28 documents for RAG\n"
     ]
    }
   ],
   "source": [
    "# Optional: Create a combined text corpus for RAG vector database\n",
    "def create_text_corpus_for_rag():\n",
    "    \"\"\"\n",
    "    Create a combined text corpus from all loaded data for RAG implementation\n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    \n",
    "    # Process Excel data\n",
    "    for name, df in excel_datasets.items():\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            # Convert DataFrame to text representation\n",
    "            text_content = f\"Dataset: {name}\\n\"\n",
    "            text_content += f\"Columns: {', '.join(df.columns)}\\n\"\n",
    "            text_content += df.to_string(max_rows=100, max_cols=10)\n",
    "            corpus.append({\n",
    "                'source': f\"excel_{name}\",\n",
    "                'content': text_content,\n",
    "                'type': 'tabular_data'\n",
    "            })\n",
    "    \n",
    "    # Process PDF extracted data\n",
    "    for name, data in pdf_datasets.items():\n",
    "        if isinstance(data, str):\n",
    "            corpus.append({\n",
    "                'source': f\"pdf_{name}\",\n",
    "                'content': data,\n",
    "                'type': 'text'\n",
    "            })\n",
    "        elif isinstance(data, dict):\n",
    "            # Convert dict to searchable text\n",
    "            text_content = f\"Document: {name}\\n\"\n",
    "            text_content += json.dumps(data, indent=2)\n",
    "            corpus.append({\n",
    "                'source': f\"pdf_{name}\",\n",
    "                'content': text_content,\n",
    "                'type': 'structured_data'\n",
    "            })\n",
    "        elif isinstance(data, pd.DataFrame):\n",
    "            text_content = f\"Dataset: {name}\\n\"\n",
    "            text_content += f\"Columns: {', '.join(data.columns)}\\n\"\n",
    "            text_content += data.to_string(max_rows=100, max_cols=10)\n",
    "            corpus.append({\n",
    "                'source': f\"pdf_{name}\",\n",
    "                'content': text_content,\n",
    "                'type': 'tabular_data'\n",
    "            })\n",
    "    \n",
    "    print(f\"Created text corpus with {len(corpus)} documents for RAG\")\n",
    "    return corpus\n",
    "\n",
    "# Create corpus for RAG\n",
    "text_corpus = create_text_corpus_for_rag()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_data_processor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
