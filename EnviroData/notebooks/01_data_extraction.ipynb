{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6c12ebf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'is_cjk_uncommon' from 'charset_normalizer.utils' (c:\\Users\\Jordan\\miniconda3\\envs\\env_data_processor\\lib\\site-packages\\charset_normalizer\\utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpdfplumber\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jordan\\miniconda3\\envs\\env_data_processor\\lib\\site-packages\\pdfplumber\\__init__.py:11\u001b[0m\n\u001b[0;32m      1\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutils\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_debug\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m ]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpdfminer\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpdfminer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpdftypes\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n",
      "File \u001b[1;32mc:\\Users\\Jordan\\miniconda3\\envs\\env_data_processor\\lib\\site-packages\\pdfminer\\pdftypes.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mccitt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ccittfaxdecode\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlzw\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lzwdecode\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpsparser\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LIT\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpsparser\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PSException\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpsparser\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PSObject\n",
      "File \u001b[1;32mc:\\Users\\Jordan\\miniconda3\\envs\\env_data_processor\\lib\\site-packages\\pdfminer\\psparser.py:22\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     Any,\n\u001b[0;32m      9\u001b[0m     BinaryIO,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     Union,\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m settings\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m choplist\n\u001b[0;32m     24\u001b[0m log \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPSException\u001b[39;00m(\u001b[38;5;167;01mException\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\Jordan\\miniconda3\\envs\\env_data_processor\\lib\\site-packages\\pdfminer\\utils.py:31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayout\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LTComponent\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcharset_normalizer\u001b[39;00m  \u001b[38;5;66;03m# For str encoding detection\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# from sys import maxint as INF doesn't work anymore under Python3, but PDF\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# still uses 32 bits ints\u001b[39;00m\n\u001b[0;32m     35\u001b[0m INF \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<<\u001b[39m \u001b[38;5;241m31\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Jordan\\miniconda3\\envs\\env_data_processor\\lib\\site-packages\\charset_normalizer\\__init__.py:24\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mCharset-Normalizer\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m:license: MIT, see LICENSE for more details.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m from_bytes, from_fp, from_path, is_binary\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m detect\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CharsetMatch, CharsetMatches\n",
      "File \u001b[1;32mc:\\Users\\Jordan\\miniconda3\\envs\\env_data_processor\\lib\\site-packages\\charset_normalizer\\api.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PathLike\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BinaryIO, List, Optional, Set, Union\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     coherence_ratio,\n\u001b[0;32m      7\u001b[0m     encoding_languages,\n\u001b[0;32m      8\u001b[0m     mb_encoding_languages,\n\u001b[0;32m      9\u001b[0m     merge_coherence_ratios,\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstant\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IANA_SUPPORTED, TOO_BIG_SEQUENCE, TOO_SMALL_SEQUENCE, TRACE\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mess_ratio\n",
      "File \u001b[1;32mc:\\Users\\Jordan\\miniconda3\\envs\\env_data_processor\\lib\\site-packages\\charset_normalizer\\cd.py:14\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter \u001b[38;5;28;01mas\u001b[39;00m TypeCounter, Dict, List, Optional, Tuple\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstant\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     FREQUENCIES,\n\u001b[0;32m      9\u001b[0m     KO_NAMES,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     ZH_NAMES,\n\u001b[0;32m     13\u001b[0m )\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_suspiciously_successive_range\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CoherenceMatches\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     is_accentuated,\n\u001b[0;32m     18\u001b[0m     is_latin,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     unicode_range,\n\u001b[0;32m     22\u001b[0m )\n",
      "File \u001b[1;32msrc\\charset_normalizer\\md.py:11\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'is_cjk_uncommon' from 'charset_normalizer.utils' (c:\\Users\\Jordan\\miniconda3\\envs\\env_data_processor\\lib\\site-packages\\charset_normalizer\\utils.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import tabula\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import pdfplumber\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f02e2010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53b7d98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Environmental Data Extractor ===\n",
      "Using Python: c:\\Users\\Jordan\\miniconda3\\envs\\env_data_processor\\python.exe\n",
      "Pandas version: 2.3.1\n",
      "Working directory: c:\\Users\\Jordan\\OneDrive - St Leonard's College\\Desktop\\Computing\\EnviroAI\\EnviroData\\notebooks\n"
     ]
    }
   ],
   "source": [
    "# Setup logging\n",
    "log_path = Path(os.getenv('LOG_PATH', './logs'))\n",
    "log_path.mkdir(exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_path / 'data_extraction.log'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"=== Environmental Data Extractor ===\")\n",
    "print(f\"Using Python: {sys.executable}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e225e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-21 11:27:46,382 - __main__ - INFO - Initialized DataExtractor with path: data\\raw\n"
     ]
    }
   ],
   "source": [
    "class DataExtractor:\n",
    "    def __init__(self, data_path=None):\n",
    "        self.data_path = Path(data_path) if data_path else Path(os.getenv('RAW_DATA_PATH', './data/raw'))\n",
    "        self.excel_path = self.data_path / 'excel'\n",
    "        self.pdf_path = self.data_path / 'pdf'\n",
    "        \n",
    "        logger.info(f\"Initialized DataExtractor with path: {self.data_path}\")\n",
    "        \n",
    "    def list_files(self):\n",
    "        \"\"\"List all Excel and PDF files in the data directories\"\"\"\n",
    "        excel_files = list(self.excel_path.glob('*.xlsx')) + list(self.excel_path.glob('*.xls'))\n",
    "        pdf_files = list(self.pdf_path.glob('*.pdf'))\n",
    "        \n",
    "        print(f\"\\nFound {len(excel_files)} Excel files:\")\n",
    "        for file in excel_files:\n",
    "            print(f\"  - {file.name}\")\n",
    "            \n",
    "        print(f\"\\nFound {len(pdf_files)} PDF files:\")\n",
    "        for file in pdf_files:\n",
    "            print(f\"  - {file.name}\")\n",
    "            \n",
    "        return excel_files, pdf_files\n",
    "        \n",
    "    def extract_excel_data(self, file_path):\n",
    "        \"Extract data from Excel files\"\n",
    "        excel_data = {}\n",
    "        try:\n",
    "            logger.info(f\"Extracting Excel data from: {file_path.name}\")\n",
    "            \n",
    "            # Get file info\n",
    "            file_size = file_path.stat().st_size / (1024*1024)  # Size in MB\n",
    "            if file_size > float(os.getenv('MAX_FILE_SIZE_MB', 100)):\n",
    "                logger.warning(f\"File {file_path.name} is large ({file_size:.1f}MB)\")\n",
    "            \n",
    "            # Handle multiple sheets\n",
    "            excel_file = pd.ExcelFile(file_path)\n",
    "            sheets_data = {}\n",
    "            \n",
    "            print(f\"\\nProcessing {file_path.name} with {len(excel_file.sheet_names)} sheets:\")\n",
    "            \n",
    "            for sheet in excel_file.sheet_names:\n",
    "                print(f\"  - Reading sheet: {sheet}\")\n",
    "                try:\n",
    "                    df = pd.read_excel(file_path, sheet_name=sheet)\n",
    "                    sheets_data[sheet] = {\n",
    "                        'data': df,\n",
    "                        'shape': df.shape,\n",
    "                        'columns': list(df.columns),\n",
    "                        'source_file': file_path.name,\n",
    "                        'sheet_name': sheet,\n",
    "                        'extraction_date': datetime.now().isoformat()\n",
    "                    }\n",
    "                    logger.info(f\"Sheet '{sheet}': {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error reading sheet '{sheet}': {e}\")\n",
    "                    continue\n",
    "                    \n",
    "            return sheets_data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting Excel data from {file_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_pdf_tables(self, file_path):\n",
    "        \"\"\"Extract tables from PDF files using tabula-py (conda installed)\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Extracting PDF tables from: {file_path.name}\")\n",
    "            \n",
    "            # Check if Java is available (required for tabula-py)\n",
    "            try:\n",
    "                import subprocess\n",
    "                subprocess.run(['java', '-version'], capture_output=True, check=True)\n",
    "            except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "                logger.warning(\"Java not found. Tabula-py requires Java for PDF table extraction.\")\n",
    "                return self.extract_pdf_tables_fallback(file_path)\n",
    "            \n",
    "            # Using tabula for table extraction\n",
    "            tables = tabula.read_pdf(\n",
    "                str(file_path), \n",
    "                pages='all', \n",
    "                multiple_tables=True,\n",
    "                pandas_options={'header': 0}\n",
    "            )\n",
    "            \n",
    "            processed_tables = []\n",
    "            for i, table in enumerate(tables):\n",
    "                if not table.empty:\n",
    "                    processed_tables.append({\n",
    "                        'data': table,\n",
    "                        'shape': table.shape,\n",
    "                        'columns': list(table.columns),\n",
    "                        'source_file': file_path.name,\n",
    "                        'table_index': i,\n",
    "                        'extraction_date': datetime.now().isoformat()\n",
    "                    })\n",
    "            \n",
    "            logger.info(f\"Extracted {len(processed_tables)} tables from {file_path.name}\")\n",
    "            return processed_tables\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting PDF tables from {file_path}: {e}\")\n",
    "            return self.extract_pdf_tables_fallback(file_path)\n",
    "    \n",
    "    def extract_pdf_tables_fallback(self, file_path):\n",
    "        \"\"\"Fallback PDF table extraction using pdfplumber\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Using pdfplumber fallback for: {file_path.name}\")\n",
    "            \n",
    "            tables = []\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                for page_num, page in enumerate(pdf.pages):\n",
    "                    page_tables = page.extract_tables()\n",
    "                    for table_num, table in enumerate(page_tables):\n",
    "                        if table and len(table) > 1:  # Must have header and at least one row\n",
    "                            # Convert to DataFrame\n",
    "                            df = pd.DataFrame(table[1:], columns=table[0])\n",
    "                            tables.append({\n",
    "                                'data': df,\n",
    "                                'shape': df.shape,\n",
    "                                'columns': list(df.columns),\n",
    "                                'source_file': file_path.name,\n",
    "                                'page_number': page_num + 1,\n",
    "                                'table_index': table_num,\n",
    "                                'extraction_date': datetime.now().isoformat()\n",
    "                            })\n",
    "            \n",
    "            logger.info(f\"Extracted {len(tables)} tables using pdfplumber from {file_path.name}\")\n",
    "            return tables\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error with pdfplumber extraction from {file_path}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def extract_pdf_text(self, file_path):\n",
    "        \"\"\"Extract text from PDF files\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Extracting text from: {file_path.name}\")\n",
    "            \n",
    "            # Try PyPDF2 first\n",
    "            try:\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    text = \"\"\n",
    "                    for page_num, page in enumerate(reader.pages):\n",
    "                        page_text = page.extract_text()\n",
    "                        text += f\"\\n--- Page {page_num + 1} ---\\n{page_text}\"\n",
    "                    \n",
    "                return {\n",
    "                    'text': text,\n",
    "                    'pages': len(reader.pages),\n",
    "                    'source_file': file_path.name,\n",
    "                    'extraction_method': 'PyPDF2',\n",
    "                    'extraction_date': datetime.now().isoformat()\n",
    "                    }\n",
    "            except Exception as pypdf_error:\n",
    "                logger.warning(f\"PyPDF2 failed, trying pdfplumber: {pypdf_error}\")\n",
    "                \n",
    "                # Fallback to pdfplumber\n",
    "                with pdfplumber.open(file_path) as pdf:\n",
    "                    text = \"\"\n",
    "                    for page_num, page in enumerate(pdf.pages):\n",
    "                        page_text = page.extract_text()\n",
    "                        if page_text:\n",
    "                            text += f\"\\n--- Page {page_num + 1} ---\\n{page_text}\"\n",
    "                    \n",
    "                return {\n",
    "                    'text': text,\n",
    "                    'pages': len(pdf.pages),\n",
    "                    'source_file': file_path.name,\n",
    "                    'extraction_method': 'pdfplumber',\n",
    "                    'extraction_date': datetime.now().isoformat()\n",
    "                    }\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting text from {file_path}: {e}\")\n",
    "            return None\n",
    "        \n",
    "extractor = DataExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7747adce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_text(self, file_path):\n",
    "        \"\"\"Extract text from PDF files\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Extracting text from: {file_path.name}\")\n",
    "            \n",
    "            # Try PyPDF2 first\n",
    "            try:\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    text = \"\"\n",
    "                    for page_num, page in enumerate(reader.pages):\n",
    "                        page_text = page.extract_text()\n",
    "                        text += f\"\\n--- Page {page_num + 1} ---\\n{page_text}\"\n",
    "                    \n",
    "                return {\n",
    "                    'text': text,\n",
    "                    'pages': len(reader.pages),\n",
    "                    'source_file': file_path.name,\n",
    "                    'extraction_method': 'PyPDF2',\n",
    "                    'extraction_date': datetime.now().isoformat()\n",
    "                    }\n",
    "            except Exception as pypdf_error:\n",
    "                logger.warning(f\"PyPDF2 failed, trying pdfplumber: {pypdf_error}\")\n",
    "                \n",
    "                # Fallback to pdfplumber\n",
    "                with pdfplumber.open(file_path) as pdf:\n",
    "                    text = \"\"\n",
    "                    for page_num, page in enumerate(pdf.pages):\n",
    "                        page_text = page.extract_text()\n",
    "                        if page_text:\n",
    "                            text += f\"\\n--- Page {page_num + 1} ---\\n{page_text}\"\n",
    "                    \n",
    "                return {\n",
    "                    'text': text,\n",
    "                    'pages': len(pdf.pages),\n",
    "                    'source_file': file_path.name,\n",
    "                    'extraction_method': 'pdfplumber',\n",
    "                    'extraction_date': datetime.now().isoformat()\n",
    "                    }\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting text from {file_path}: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17df8a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SCANNING FOR FILES\n",
      "==================================================\n",
      "\n",
      "Found 2 Excel files:\n",
      "  - ghg-conversion-factors-2023-condensed-set-update.xlsx\n",
      "  - ghg-emission-factors-hub-2025.xlsx\n",
      "\n",
      "Found 1 PDF files:\n",
      "  - ZyPDF.pdf\n"
     ]
    }
   ],
   "source": [
    "# List available files\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SCANNING FOR FILES\")\n",
    "print(\"=\"*50)\n",
    "excel_files, pdf_files = extractor.list_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd12d390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXTRACTING EXCEL DATA\n",
      "2025-08-14 10:34:03,119 - __main__ - INFO - Extracting Excel data from: ghg-conversion-factors-2023-condensed-set-update.xlsx\n",
      "\n",
      "Processing ghg-conversion-factors-2023-condensed-set-update.xlsx with 26 sheets:\n",
      "  - Reading sheet: Introduction\n",
      "2025-08-14 10:34:04,706 - __main__ - INFO - Sheet 'Introduction': 43 rows, 4 columns\n",
      "  - Reading sheet: What's new\n",
      "2025-08-14 10:34:05,002 - __main__ - INFO - Sheet 'What's new': 49 rows, 4 columns\n",
      "  - Reading sheet: Index\n",
      "2025-08-14 10:34:05,318 - __main__ - INFO - Sheet 'Index': 103 rows, 4 columns\n",
      "  - Reading sheet: Fuels\n",
      "2025-08-14 10:34:05,563 - __main__ - INFO - Sheet 'Fuels': 159 rows, 8 columns\n",
      "  - Reading sheet: Bioenergy\n",
      "2025-08-14 10:34:05,844 - __main__ - INFO - Sheet 'Bioenergy': 83 rows, 6 columns\n",
      "  - Reading sheet: Refrigerant & other\n",
      "2025-08-14 10:34:06,085 - __main__ - INFO - Sheet 'Refrigerant & other': 229 rows, 7 columns\n",
      "  - Reading sheet: Passenger vehicles\n",
      "2025-08-14 10:34:06,425 - __main__ - INFO - Sheet 'Passenger vehicles': 118 rows, 35 columns\n",
      "  - Reading sheet: SECR kWh pass & delivery vehs\n",
      "2025-08-14 10:34:06,719 - __main__ - INFO - Sheet 'SECR kWh pass & delivery vehs': 137 rows, 11 columns\n",
      "  - Reading sheet: UK electricity\n",
      "2025-08-14 10:34:07,010 - __main__ - INFO - Sheet 'UK electricity': 31 rows, 8 columns\n",
      "  - Reading sheet: UK electricity for EVs\n",
      "2025-08-14 10:34:07,496 - __main__ - INFO - Sheet 'UK electricity for EVs': 86 rows, 11 columns\n",
      "  - Reading sheet: SECR kWh UK electricity for EVs\n",
      "2025-08-14 10:34:07,768 - __main__ - INFO - Sheet 'SECR kWh UK electricity for EVs': 77 rows, 6 columns\n",
      "  - Reading sheet: Transmission and distribution\n",
      "2025-08-14 10:34:08,020 - __main__ - INFO - Sheet 'Transmission and distribution': 32 rows, 8 columns\n",
      "  - Reading sheet: UK electricity T&D for EVs\n",
      "2025-08-14 10:34:09,063 - __main__ - INFO - Sheet 'UK electricity T&D for EVs': 88 rows, 11 columns\n",
      "  - Reading sheet: Water supply\n",
      "2025-08-14 10:34:09,485 - __main__ - INFO - Sheet 'Water supply': 22 rows, 6 columns\n",
      "  - Reading sheet: Water treatment\n",
      "2025-08-14 10:34:09,971 - __main__ - INFO - Sheet 'Water treatment': 19 rows, 6 columns\n",
      "  - Reading sheet: Material use\n",
      "2025-08-14 10:34:10,296 - __main__ - INFO - Sheet 'Material use': 92 rows, 14 columns\n",
      "  - Reading sheet: Waste disposal\n",
      "2025-08-14 10:34:10,685 - __main__ - INFO - Sheet 'Waste disposal': 97 rows, 14 columns\n",
      "  - Reading sheet: Business travel- air\n",
      "2025-08-14 10:34:10,890 - __main__ - INFO - Sheet 'Business travel- air': 55 rows, 12 columns\n",
      "  - Reading sheet: Business travel- sea\n",
      "2025-08-14 10:34:11,087 - __main__ - INFO - Sheet 'Business travel- sea': 24 rows, 7 columns\n",
      "  - Reading sheet: Business travel- land\n",
      "2025-08-14 10:34:11,401 - __main__ - INFO - Sheet 'Business travel- land': 102 rows, 35 columns\n",
      "  - Reading sheet: Freighting goods\n",
      "2025-08-14 10:34:11,634 - __main__ - INFO - Sheet 'Freighting goods': 184 rows, 31 columns\n",
      "  - Reading sheet: Hotel stay\n",
      "2025-08-14 10:34:11,889 - __main__ - INFO - Sheet 'Hotel stay': 90 rows, 7 columns\n",
      "  - Reading sheet: Homeworking\n",
      "2025-08-14 10:34:12,135 - __main__ - INFO - Sheet 'Homeworking': 30 rows, 6 columns\n",
      "  - Reading sheet: Conversions\n",
      "2025-08-14 10:34:12,350 - __main__ - INFO - Sheet 'Conversions': 64 rows, 8 columns\n",
      "  - Reading sheet: Fuel properties\n",
      "2025-08-14 10:34:12,619 - __main__ - INFO - Sheet 'Fuel properties': 62 rows, 11 columns\n",
      "  - Reading sheet: Haul definition\n",
      "2025-08-14 10:34:12,823 - __main__ - INFO - Sheet 'Haul definition': 219 rows, 3 columns\n",
      "\n",
      "Successfully extracted data from ghg-conversion-factors-2023-condensed-set-update.xlsx\n",
      "Sheet 'Introduction': (43, 4)\n",
      "Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3']\n",
      "Sheet 'What's new': (49, 4)\n",
      "Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3']\n",
      "Sheet 'Index': (103, 4)\n",
      "Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3']\n",
      "Sheet 'Fuels': (159, 8)\n",
      "Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "Sheet 'Bioenergy': (83, 6)\n",
      "Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "Sheet 'Refrigerant & other': (229, 7)\n",
      "Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "Sheet 'Passenger vehicles': (118, 35)\n",
      "Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "Sheet 'SECR kWh pass & delivery vehs': (137, 11)\n",
      "Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "Sheet 'UK electricity': (31, 8)\n",
      "Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "Sheet 'UK electricity for EVs': (86, 11)\n",
      "Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "Sheet 'SECR kWh UK electricity for EVs': (77, 6)\n",
      "Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "Sheet 'Transmission and distribution': (32, 8)\n",
      "Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "Sheet 'UK electricity T&D for EVs': (88, 11)\n",
      "Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "Sheet 'Water supply': (22, 6)\n",
      "Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "Sheet 'Water treatment': (19, 6)\n",
      "Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "Sheet 'Material use': (92, 14)\n",
      "Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "Sheet 'Waste disposal': (97, 14)\n",
      "Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "Sheet 'Business travel- air': (55, 12)\n",
      "Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "Sheet 'Business travel- sea': (24, 7)\n",
      "Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "Sheet 'Business travel- land': (102, 35)\n",
      "Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "Sheet 'Freighting goods': (184, 31)\n",
      "Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "Sheet 'Hotel stay': (90, 7)\n",
      "Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "Sheet 'Homeworking': (30, 6)\n",
      "Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "Sheet 'Conversions': (64, 8)\n",
      "Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "Sheet 'Fuel properties': (62, 11)\n",
      "Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n",
      "Sheet 'Haul definition': (219, 3)\n",
      "Columns: ['UK Government GHG Conversion Factors for Company Reporting', 'Unnamed: 1', 'Unnamed: 2']\n",
      "2025-08-14 10:34:12,824 - __main__ - INFO - Extracting Excel data from: ghg-emission-factors-hub-2025.xlsx\n",
      "\n",
      "Processing ghg-emission-factors-hub-2025.xlsx with 1 sheets:\n",
      "  - Reading sheet: Emission Factors Hub\n",
      "2025-08-14 10:34:13,142 - __main__ - INFO - Sheet 'Emission Factors Hub': 590 rows, 11 columns\n",
      "\n",
      "Successfully extracted data from ghg-emission-factors-hub-2025.xlsx\n",
      "Sheet 'Emission Factors Hub': (590, 11)\n",
      "Columns: ['Unnamed: 0', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jordan\\miniconda3\\envs\\env_data_processor\\lib\\site-packages\\openpyxl\\worksheet\\header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n",
      "c:\\Users\\Jordan\\miniconda3\\envs\\env_data_processor\\lib\\site-packages\\openpyxl\\worksheet\\_read_only.py:85: UserWarning: Unknown extension is not supported and will be removed\n",
      "  for idx, row in parser.parse():\n"
     ]
    }
   ],
   "source": [
    "if excel_files:\n",
    "    print(\"EXTRACTING EXCEL DATA\")\n",
    "    \n",
    "    for excel_file in excel_files:\n",
    "        excel_data = extractor.extract_excel_data(excel_file)\n",
    "        \n",
    "        if excel_data:\n",
    "            print(f\"\\nSuccessfully extracted data from {excel_file.name}\")\n",
    "            for sheet_name, sheet_info in excel_data.items():\n",
    "                print(f\"Sheet '{sheet_name}': {sheet_info['shape']}\")\n",
    "                print(f\"Columns: {sheet_info['columns'][:5]}{'...' if len(sheet_info['columns']) > 5 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71082391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EXTRACTING PDF DATA\n",
      "==================================================\n",
      "2025-08-21 11:28:31,871 - __main__ - INFO - Extracting PDF tables from: ZyPDF.pdf\n",
      "2025-08-21 11:28:36,801 - tabula.io - WARNING - Got stderr: Aug 21, 2025 11:28:34 AM org.apache.pdfbox.contentstream.PDFStreamEngine operatorException\n",
      "SEVERE: Cannot read JPEG2000 image: Java Advanced Imaging (JAI) Image I/O Tools are not installed\n",
      "\n",
      "2025-08-21 11:28:36,805 - __main__ - ERROR - Error extracting PDF tables from data\\raw\\pdf\\ZyPDF.pdf: 'utf-8' codec can't decode byte 0x92 in position 750: invalid start byte\n",
      "2025-08-21 11:28:36,808 - __main__ - INFO - Using pdfplumber fallback for: ZyPDF.pdf\n",
      "2025-08-21 11:28:36,811 - __main__ - ERROR - Error with pdfplumber extraction from data\\raw\\pdf\\ZyPDF.pdf: name 'pdfplumber' is not defined\n",
      "\n",
      "==================================================\n",
      "EXTRACTION COMPLETE\n",
      "==================================================\n",
      "Check the logs directory for detailed extraction logs.\n",
      "Next: Run the data cleaning notebook (02_data_cleaning.ipynb)\n"
     ]
    }
   ],
   "source": [
    "if pdf_files:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"EXTRACTING PDF DATA\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    first_pdf = pdf_files[0]\n",
    "    pdf_tables = extractor.extract_pdf_tables(first_pdf)\n",
    "    \n",
    "    if pdf_tables:\n",
    "        print(f\"\\nSuccessfully extracted {len(pdf_tables)} tables from {first_pdf.name}\")\n",
    "        for i, table_info in enumerate(pdf_tables):\n",
    "            print(f\"Table {i+1}: {table_info['shape']}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"EXTRACTION COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(\"Check the logs directory for detailed extraction logs.\")\n",
    "print(\"Next: Run the data cleaning notebook (02_data_cleaning.ipynb)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_data_processor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
